{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune for xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from kerastuner.tuners import BayesianOptimization\n",
    "\n",
    "seed = 444 # random number seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(the_pred, \n",
    "                          the_true, \n",
    "                          save_plot=False, \n",
    "                          the_path=None):\n",
    "    # generate confusion matrix\n",
    "    confusion_matrix = metrics.confusion_matrix(the_pred, the_true)\n",
    "    \n",
    "    # generate the plot\n",
    "    plot_df = pd.DataFrame(confusion_matrix, index=[\"0\", \"1\"], columns=[\"0\", \"1\"])\n",
    "    fig, ax = plt.subplots(figsize=(6,5))\n",
    "    sns.heatmap(plot_df, annot=True, cmap=\"YlGnBu\", fmt=\"g\", ax=ax)\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.xlabel(\"True\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    \n",
    "    # if save plot\n",
    "    if save_plot == True:\n",
    "        fig.savefig(the_path, dpi=500)   \n",
    "\n",
    "def model_performance(y_test,y_pred):\n",
    "    res = pd.DataFrame()\n",
    "    res['accuracy'] = [metrics.accuracy_score(y_test,y_pred)]\n",
    "    res['Down_precision'] = [metrics.classification_report(y_test,y_pred,\n",
    "                                                   output_dict=True)['0']['precision']]\n",
    "    res['Up_precision'] = [metrics.classification_report(y_test,y_pred,output_dict=True)['1']['precision']]\n",
    "    res['f1_score'] = [metrics.f1_score(y_test,y_pred,average='weighted')]\n",
    "    res['recall_score'] = [metrics.recall_score(y_test,y_pred,average='weighted')]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.dirname(os.getcwd())\n",
    "data_path = os.path.join(data_path, \"Data/features.csv\")\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "data = data[data[\"Date\"] < \"2020-01-01\"]\n",
    "del data[\"Date\"]\n",
    "data_y = data.pop(\"direction\").values\n",
    "data_x = data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data: 0 mean and unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract categorical features\n",
    "condition_macd = data.columns == \"macd\"\n",
    "macd = data_x[:, condition_macd]\n",
    "data_x_nomalized = data_x[:, [not i for i in condition_macd]]  # all features except macd\n",
    "\n",
    "# normalized\n",
    "scaler = StandardScaler()\n",
    "data_x_nomalized =  scaler.fit_transform(data_x_nomalized)\n",
    "\n",
    "# append macd\n",
    "data_x_nomalized = np.hstack((data_x_nomalized, macd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Boosting tree: xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data to xgboost format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-validation split\n",
    "trainX, validationX, trainY, validationY = train_test_split(\n",
    "                                                                data_x_nomalized,\n",
    "                                                                data_y,\n",
    "                                                                train_size=0.8,\n",
    "                                                                random_state=seed\n",
    "                                                            )\n",
    "# convert to xgboost format\n",
    "trainX_xg = xgb.DMatrix(trainX, label=trainY) # TODO delete\n",
    "validationX_xg = xgb.DMatrix(validationX, label=validationY) # TODO delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the grid search to find the best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up param grid\n",
    "eta = [0.1, 0.015, 0.01, 0.0015, 0.001]\n",
    "gamma = [0.5, 1.0, 1.5, 2.0, 2.5, 0]\n",
    "max_depth = [i for i in range(1, 20)]\n",
    "boosting_rounds = [i for i in range(2, 51)]\n",
    "parameters = {\n",
    "    \"n_estimators\": boosting_rounds,\n",
    "    \"learning_rate\": eta,\n",
    "    \"gamma\": gamma,\n",
    "    \"max_depth\": max_depth\n",
    "}\n",
    "\n",
    "# set up classifier\n",
    "xgboost_classifier = xgb.XGBClassifier(objective=\"binary:logistic\")\n",
    "xg_gridSearch = GridSearchCV(\n",
    "    xgboost_classifier,\n",
    "    parameters,\n",
    "    scoring=\"accuracy\",\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# find params\n",
    "xg_gridSearch.fit(trainX, trainY)\n",
    "\n",
    "# extract tuned model\n",
    "xgboost_tuned = xg_gridSearch.best_estimator_\n",
    "with open(os.path.join(os.getcwd(), \"models/xgboost.pickle\"), 'wb') as f:\n",
    "    pickle.dump(xgboost_tuned, f)\n",
    "\n",
    "# print\n",
    "print(\"Best Params:\")\n",
    "print(xg_gridSearch.best_params_)\n",
    "print(\"Best Score:\")\n",
    "print(xg_gridSearch.best_score_)\n",
    "\n",
    "# Best Params:\n",
    "# {'gamma': 0.5, 'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 24}\n",
    "# Best Score:\n",
    "# 0.7090909090909092"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model performance on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load pickle\n",
    "with open(os.path.join(os.getcwd(), \"models/xgboost.pickle\"), \"rb\") as f:\n",
    "    xgboost_tuned = pickle.load(f)\n",
    "\n",
    "# generate prediction on validation dataset\n",
    "xgboost_pred = xgboost_tuned.predict(validationX)\n",
    "xgboost_pred = [1 if i >= 0.5 else 0 for i in xgboost_pred]\n",
    "\n",
    "# confusion matrix\n",
    "xgboost_confusion_matrix = plot_confusion_matrix(xgboost_pred, \n",
    "                                                 validationY,\n",
    "                                                save_plot=True,\n",
    "                                                the_path=os.path.join(os.getcwd(), \"confusion_matrix/xgboost.png\"))\n",
    "\n",
    "# model performance\n",
    "xgboost_performance = model_performance(validationY, xgboost_pred)\n",
    "print(xgboost_performance)\n",
    "\n",
    "#    accuracy  Down_precision  Up_precision  f1_score  recall_score\n",
    "# 0  0.547619        0.666667      0.527778  0.481481      0.547619"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "data_shape = data_x_nomalized.shape\n",
    "# one-hot encoding for macd\n",
    "macd = data_x_nomalized[:, data_shape[1] - 1]\n",
    "data_x_nomalized = data_x_nomalized[:, :data_shape[1] - 1]\n",
    "macd = to_categorical(macd, num_classes=2)\n",
    "data_x_nomalized = np.hstack((data_x_nomalized, macd))\n",
    "# one-hot encoding for label\n",
    "data_y_encoded = to_categorical(data_y, num_classes=2)\n",
    "\n",
    "# split to train and test\n",
    "trainX, validationX, trainY, validationY = train_test_split(\n",
    "                                                                data_x_nomalized,\n",
    "                                                                data_y_encoded,\n",
    "                                                                train_size=0.8,\n",
    "                                                                random_state=seed\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special steps for LSTM models: reshape input to be [samples, time steps, features]\n",
    "trainX = trainX.reshape(trainX.shape[0], 1, trainX.shape[1])\n",
    "validationX = validationX.reshape(validationX.shape[0], 1, validationX.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build model functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "def build_model_LSTM(hp):\n",
    "    # model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # input layer\n",
    "    input_num_unit= hp.Int(\"input_layer_units\", 50, 100, 10)\n",
    "    input_drop_ratio = hp.Float(\"input_layer_dropRatio\", 0.0, 0.5, 0.1)\n",
    "    model.add(LSTM(units=input_num_unit, input_shape=(1, 27), return_sequences=True))\n",
    "    model.add(Dropout(input_drop_ratio))\n",
    "\n",
    "    # layer set\n",
    "    # LSTM layers set 1\n",
    "    num_layers_1 = hp.Int(\"num_layers_set_1\", 0, 3)\n",
    "    num_units_1 = hp.Int(\"num_units_set_1\", 50, 100, 10)\n",
    "    dropRatio_1 = hp.Float(\"dropRatio_set_1\", 0.0, 0.5, 0.1)\n",
    "    for i in range(num_layers_1):\n",
    "        model.add(LSTM(units=num_units_1, return_sequences=True))\n",
    "        model.add(Dropout(dropRatio_1))\n",
    "\n",
    "    # LSTM layer set 2\n",
    "    num_layers_2 = hp.Int(\"num_layers_set_2\", 0, 3)\n",
    "    num_units_2 = hp.Int(\"num_units_set_2\", 10, 50, 10)\n",
    "    dropRatio_2 = hp.Float(\"dropRatio_set_2\", 0.0, 0.5, 0.1)\n",
    "    for i in range(num_layers_2):\n",
    "        model.add(LSTM(units=num_units_2, return_sequences=True))\n",
    "        model.add(Dropout(dropRatio_2))\n",
    "    \n",
    "    # LSTM last layer\n",
    "    num_units_3 = hp.Int(\"num_units_set_3\", 10, 50, 10)\n",
    "    model.add(LSTM(units=num_units_3, return_sequences=False))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "    # tune learning rate\n",
    "    lr = hp.Choice(\"lr\", values=[0.1, 0.015, 0.01, 0.0015, 0.001])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=lr), loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Optimization, tune best hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project /Users/lihaohang/Desktop/FE-595-Final/Models_train/tune_LSTM/oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from /Users/lihaohang/Desktop/FE-595-Final/Models_train/tune_LSTM/tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner_LSTM = BayesianOptimization(\n",
    "    build_model_LSTM,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=100,\n",
    "    executions_per_trial=3,\n",
    "    directory=os.getcwd(),\n",
    "    project_name=\"tune_LSTM\",\n",
    "    overwrite = False\n",
    ")\n",
    "\n",
    "tuner_LSTM.search(\n",
    "    x=trainX,\n",
    "    y=trainY,\n",
    "    verbose=1,\n",
    "    epochs=100,\n",
    "    batch_size=30,\n",
    "    validation_data=(validationX, validationY)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore the test and train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = trainX.reshape(trainX.shape[0], trainX.shape[2])\n",
    "validationX = validationX.reshape(validationX.shape[0], validationX.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build model function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_ANN(hp):\n",
    "    # model\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(27,)))\n",
    "\n",
    "    # tune layers\n",
    "    # dense layers set 1\n",
    "    num_layers_1 = hp.Int(\"n_layers_1\", 0, 3)\n",
    "    num_units_1 = hp.Int(\"n_units_1\", 50, 100, 10)\n",
    "    for i in range(num_layers_1):\n",
    "        model.add(Dense(num_units_1))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "    # dense layer set 2\n",
    "    num_layers_2 = hp.Int(\"n_layers_2\", 0, 3)\n",
    "    num_units_2 = hp.Int(\"n_units_2\", 10, 50, 10)\n",
    "    for i in range(num_layers_2):\n",
    "        model.add(Dense(num_units_2))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "    # output layer\n",
    "    model.add(Dense(2, activation=\"sigmoid\"))\n",
    "\n",
    "    # tune learning rate\n",
    "    lr = hp.Choice(\"lr\", values=[0.1, 0.015, 0.01, 0.0015, 0.001])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=lr), loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Optimization, tune best hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_ANN = BayesianOptimization(\n",
    "    build_model_ANN,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=100,\n",
    "    executions_per_trial=3,\n",
    "    directory=os.getcwd(),\n",
    "    project_name=\"tune_ANN\",\n",
    "    overwrite = False\n",
    ")\n",
    "\n",
    "tuner_ANN.search(\n",
    "    x=trainX,\n",
    "    y=trainY,\n",
    "    verbose=1,\n",
    "    epochs=100,\n",
    "    batch_size=30,\n",
    "    validation_data=(validationX, validationY)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrive result\n",
    "tuner_ANN.get_best_hyperparameters()[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_ANN.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
